{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AleTrail BeerAdvocate Scraper\n",
    "\n",
    "## Cache pages from www.beeradvocate.com locally for further scraping\n",
    "\n",
    "By: Mike Beaumier -- Fellow, Insight Data Science\n",
    "\n",
    " - Email: [michael.beaumier@gmail.com](mailto:michael.beaumier@gmail.com)\n",
    " - Twitter: [@jollyhrothgar](https://twitter.com/jollyhrothgar)\n",
    " - **LinkedIn**: [Add me w/ 1 line message about our connection](https://www.linkedin.com/in/michaelbeaumier)\n",
    "\n",
    "Repositories on [github](https://www.github.com/Jollyhrothgar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import sys\n",
    "import urllib2\n",
    "import pandas\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_brewery(url,dump_directory = '.'):\n",
    "    '''\n",
    "    takes a beer-advocate URL formatted like so: www.beeradvocate.com/pages/<numeric_id>/\" \n",
    "    and dumps the page to a utf-8 text file. Uses <numeric_id> to uniquely identify the file.\n",
    "    stores these html dumps in directory argument (defualt is '.') as: \n",
    "    '<dump_directory>/index_<numeric_id>.html'\n",
    "    '''\n",
    "    \n",
    "    # Check if data has been downloaded\n",
    "    re_check = re.search('profile/(\\d+)',url)\n",
    "    print re_check.group(1)\n",
    "    if os.path.isdir(dump_directory+'/'+re_check.group(1)):\n",
    "        print \"data exists\"\n",
    "        return 1\n",
    "    else:\n",
    "        sleep(0.25)\n",
    "    print \"Scraping URL: \",url\n",
    "    headers = { 'User-Agent' : 'Mozilla/5.0'  }\n",
    "    req = urllib2.Request(url, None, headers)\n",
    "    html = urllib2.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    \n",
    "    if re.search('No beer listings.',soup.text):\n",
    "        print \"No beer listings.\"\n",
    "        return\n",
    "\n",
    "    # Get an appropriate output name from the url\n",
    "    m = re.search(r'http://www.beeradvocate.com/beer/profile/(\\d+)/',url)\n",
    "    page_id = 0\n",
    "    if not m:\n",
    "        print 'Bad match for url:',url,'\\nBailing out!'\n",
    "        return\n",
    "    else:\n",
    "        page_id = m.group(1)\n",
    "    brewery_page = soup.encode('utf-8')\n",
    "    matches = re.findall('/\\d+/\\d+',brewery_page)\n",
    "    brew_dict = {}\n",
    "    for match in matches:\n",
    "        id_match = re.search('/(\\d+)/(\\d+)',match)\n",
    "        if id_match.group(1) not in brew_dict:\n",
    "            brew_dict[id_match.group(1)] = []\n",
    "            brew_dict[id_match.group(1)].append(id_match.group(2))\n",
    "        else:\n",
    "            brew_dict[id_match.group(1)].append(id_match.group(2))\n",
    "    # okay, now we have a list of beer pages from which to scrape reviews, but we only\n",
    "    # want the 'real' beers, not suggestions from beer-advocate, so we keep the 'larger list' of beers.\n",
    "    best_key = -1\n",
    "    key_length = -1\n",
    "    for key in brew_dict:\n",
    "        if len(brew_dict[key]) > key_length:\n",
    "            best_key = key\n",
    "            key_length = len(brew_dict[key])\n",
    "    \n",
    "    # \"http://www.beeradvocate.com/beer/profile/<brewery_id>/<beer_id>/view=beer&sort=topr&start=<page_num>\"\n",
    "    beer_url_dict = {}\n",
    "    for beer_id in brew_dict[best_key]:\n",
    "        beer_url = (\n",
    "            \"http://www.beeradvocate.com/beer/profile/\"\n",
    "            +str(best_key)\n",
    "            +\"/\"\n",
    "            +str(beer_id)\n",
    "            )\n",
    "        if beer_url not in beer_url_dict:\n",
    "            beer_url_dict[beer_url] = brew_dict[best_key]\n",
    "    # Create beer review list whether or not reviews exist. If they exist, we get the top 50\n",
    "    beer_review_list = []\n",
    "    \n",
    "    for k,v in beer_url_dict.iteritems():\n",
    "        sleep(0.25)\n",
    "        beer_req = urllib2.Request(k, None, headers)\n",
    "        beer_html = urllib2.urlopen(beer_req).read()\n",
    "        beer_soup = BeautifulSoup(beer_html,'lxml')\n",
    "        reviews_search = re.search('Reviews: (\\d+)',beer_soup.text)\n",
    "        if reviews_search:\n",
    "            print \"Reviews: \",reviews_search.group(1)\n",
    "\n",
    "            ratings_counter = 0\n",
    "            while ratings_counter < int(reviews_search.group(1))+25: \n",
    "                str_counter = str(ratings_counter)\n",
    "                beer_review_list.append(k + '/view=beer&sort=top&start='+str_counter)\n",
    "                ratings_counter += 25\n",
    "        \n",
    "#     for url in beer_review_list:\n",
    "#         print url\n",
    "    print dump_directory+'/'+str(best_key)\n",
    "    try:\n",
    "        os.mkdir(dump_directory+'/'+str(best_key))\n",
    "    except OSError:\n",
    "        print \"data exists for brewery, skipping.\\n\"\n",
    "        return\n",
    "    # dump the file\n",
    "    out_file = open(dump_directory+'/'+str(best_key)+'/brewery_'+str(page_id)+'.html','w')\n",
    "    out_file.write(brewery_page)\n",
    "    out_file.close()\n",
    "    \n",
    "    beer_counter = 0\n",
    "    for beer_url in beer_review_list:\n",
    "        print \"Scraping beer: \",beer_url\n",
    "        matches = re.search('(\\d+)/(\\d+)',beer_url)\n",
    "        out_beer_file = open(dump_directory+'/'+str(best_key)+'/beer_'+matches.group(2)+\"_\"+str(beer_counter)+'.html','w')\n",
    "        sleep(0.25)\n",
    "        beer_req = urllib2.Request(beer_url,None,headers)\n",
    "        beer_html = urllib2.urlopen(beer_req).read()\n",
    "        beer_soup = BeautifulSoup(beer_html,'lxml')\n",
    "        out_beer_file.write(beer_soup.encode('utf-8'))\n",
    "        out_beer_file.close()\n",
    "        beer_counter += 1\n",
    "    print \"Done scraping\",best_key,\"!\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42860\n",
      "Scraping URL:  http://www.beeradvocate.com/beer/profile/42860/\n",
      "No beer listings.\n"
     ]
    }
   ],
   "source": [
    "#scrape_brewery('http://www.beeradvocate.com/beer/profile/863/','rescrape_data')\n",
    "scrape_brewery('http://www.beeradvocate.com/beer/profile/42860/','rescrape_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.path.isdir(\"breweries/3120\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_url_list(filename):\n",
    "    '''\n",
    "    takes the url list in filename, loads into an array, and then returns the array of URLS\n",
    "    '''\n",
    "    in_file = open(filename,'rU')\n",
    "    brewery_list = in_file.read().splitlines()\n",
    "    print 'Loaded',len(brewery_list),\"breweries.\"\n",
    "    return brewery_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    urls = load_url_list('brewery_urls.txt')\n",
    "    master_list = []\n",
    "    for url in urls:\n",
    "        scrape_brewery(url,'./retry')\n",
    "    print \"Finished scraping!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 645 breweries.\n",
      "31027\n",
      "Scraping URL:  http://www.beeradvocate.com/beer/profile/31027/\n",
      "http://www.beeradvocate.com/beer/profile/31027/90624/view=beer&sort=top&start=0\n",
      "http://www.beeradvocate.com/beer/profile/31027/90624/view=beer&sort=top&start=25\n",
      "http://www.beeradvocate.com/beer/profile/31027/174882/view=beer&sort=top&start=0\n",
      "http://www.beeradvocate.com/beer/profile/31027/174882/view=beer&sort=top&start=25\n",
      "http://www.beeradvocate.com/beer/profile/31027/107941/view=beer&sort=top&start=0\n",
      "http://www.beeradvocate.com/beer/profile/31027/107941/view=beer&sort=top&start=25\n",
      "http://www.beeradvocate.com/beer/profile/31027/93412/view=beer&sort=top&start=0\n",
      "http://www.beeradvocate.com/beer/profile/31027/93412/view=beer&sort=top&start=25\n",
      "http://www.beeradvocate.com/beer/profile/31027/93908/view=beer&sort=top&start=0\n",
      "http://www.beeradvocate.com/beer/profile/31027/93908/view=beer&sort=top&start=25\n",
      "./retry/31027\n",
      "Scraping beer:  http://www.beeradvocate.com/beer/profile/31027/90624/view=beer&sort=top&start=0\n",
      "Scraping beer:  http://www.beeradvocate.com/beer/profile/31027/90624/view=beer&sort=top&start=25\n",
      "Scraping beer: "
     ]
    }
   ],
   "source": [
    "# Standard boilerplate to call the main() function to begin\n",
    "# the program. Execute this to run everything\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
